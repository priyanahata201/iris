# -*- coding: utf-8 -*-
"""diabetes_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P3riKPC1FCE7WeJZRwAz3B_Lc50qRunL
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
warnings.filterwarnings('ignore')
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier



"""Loading diabetes"""

#Load diabetes.csv
df = pd.read_csv('diabetes.csv')
#Print the first 5 rows
df.head()

#Check for null values
df.isnull().sum()

"""No null values. All the features are numerical."""

#Print those rows
df[df['DiabetesPedigreeFunction']>1]

"""Making distributions plots for all the features except outcome"""

#Make distribution plot for pregnancies
sns.distplot(df['Pregnancies'])

#Make distribution plot for glucose
sns.distplot(df['Glucose'])

#describe the dataset
df.describe()

#Make distribution plot for BMI
sns.distplot(df['BMI'])

#find number of people with 0 BMI
len(df[df['BMI']==0])

#Print all those rows with 0 BMI
df[df['BMI']==0]

#display the number of rows
df.shape[0]

#Print the Median of BMI
median_BMI=df['BMI'].median()
print(median_BMI)

#Replace all the 0.0 BMI values with median_BMI
df['BMI']=df['BMI'].replace(0,median_BMI)

#checking again
#Print the number of rows with 0 BMI
len(df[df['BMI']==0])

sns.distplot(df['BMI'])

#Print the number or rows with 0 Glucose
len(df[df['Glucose']==0])

#Print the median of Glucose
median_Glucose=df['Glucose'].median()
print(median_Glucose)

#Replace all the 0.0 Glucose values with median_Glucose
df['Glucose']=df['Glucose'].replace(0,median_Glucose)

len(df[df['Glucose']==0])

#make a distribution plot of blood pressure
sns.distplot(df['BloodPressure'])

#Find median of all the remaining features
median_BloodPressure=df['BloodPressure'].median()
median_SkinThickness=df['SkinThickness'].median()
median_Insulin=df['Insulin'].median()
#Print them
print(median_BloodPressure)
print(median_SkinThickness)
print(median_Insulin)

#Print the number or rows with 0 Blood Pressure
len(df[df['BloodPressure']==0])

#Print the number or rows with 0 Skin Thickness
len(df[df['SkinThickness']==0])

#Print the number or rows with 0 Insulin
len(df[df['Insulin']==0])

#For every feature, replace 0 values with their respective median
df['BloodPressure']=df['BloodPressure'].replace(0,median_BloodPressure)
df['SkinThickness']=df['SkinThickness'].replace(0,median_SkinThickness)
df['Insulin']=df['Insulin'].replace(0,median_Insulin)

#Now again check for Blood Pressure the number of rows with 0 values
len(df[df['BloodPressure']==0])

#Now again check for Skin Thickess the number of rows with 0 values
len(df[df['SkinThickness']==0])

#Now again check for Insulin the number of rows with 0 values
len(df[df['Insulin']==0])

#Make a distribution plot for age
sns.distplot(df['Age'])

#display dataset description
df.describe()

#Print number of rows with Diabetes Pedigree Function > 1
len(df[df['DiabetesPedigreeFunction']>1])

#Print those rows
df[df['DiabetesPedigreeFunction']>1]

scaler = StandardScaler()
df['DiabetesPedigreeFunction_scaled'] = scaler.fit_transform(df[['DiabetesPedigreeFunction']])

#Print those rows
df[df['DiabetesPedigreeFunction_scaled']>1]

#check for multicollinearity
df.corr()
#Plot it
sns.heatmap(df.corr(),annot=True)

# Select only the features (exclude target 'Outcome')
features = df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',
               'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']]

# Add constant term for intercept in statsmodels
X = sm.add_constant(features)

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data['Feature'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Print VIF values
print(vif_data)

"""No multicolinearity"""

#Standardize all the features
scaler = StandardScaler()
df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',
               'Insulin', 'BMI', 'DiabetesPedigreeFunction_scaled', 'Age']] = scaler.fit_transform (df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',
               'Insulin', 'BMI', 'DiabetesPedigreeFunction_scaled', 'Age']])

#remove the diabetespredigreefunction from the table
df=df.drop('DiabetesPedigreeFunction',axis=1)

#print first 5 rows of the dataset
df.head()

df.describe()

#Split the data into training and testing
X = df.drop('Outcome', axis=1)
y = df['Outcome']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Logistic Regression Model"""

# 1. Select features and target
X = df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',
         'Insulin', 'BMI', 'DiabetesPedigreeFunction_scaled', 'Age']]
y = df['Outcome']

# 2. Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 3. Feature scaling (important for Logistic Regression)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Build and train the Logistic Regression model
logreg_model = LogisticRegression(random_state=42, max_iter=1000)
logreg_model.fit(X_train_scaled, y_train)

# 5. Predictions
y_pred = logreg_model.predict(X_test_scaled)

# 6. Evaluation
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nAccuracy Score:", accuracy_score(y_test, y_pred))

"""SVM Model"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Example setup:
# df = pd.read_csv('your_file.csv')

X = df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',
         'Insulin', 'BMI', 'DiabetesPedigreeFunction_scaled', 'Age']]
y = df['Outcome']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

svm_model = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)
svm_model.fit(X_train_scaled, y_train)

y_pred = svm_model.predict(X_test_scaled)

# âœ… Correct imports now ensure no error:
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nAccuracy Score:", accuracy_score(y_test, y_pred))

"""Random Forest Model"""

# 1. Select features and target
X = df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',
         'Insulin', 'BMI', 'DiabetesPedigreeFunction_scaled', 'Age']]
y = df['Outcome']

# 2. Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 3. Build and train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)  # Note: Scaling not needed for Random Forest

# 4. Predictions
y_pred = rf_model.predict(X_test)

# 5. Evaluation
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nAccuracy Score:", accuracy_score(y_test, y_pred))

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier

# Scale features (important for KNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train_scaled, y_train)

y_pred_knn = knn_model.predict(X_test_scaled)

print("KNN Confusion Matrix:\n", confusion_matrix(y_test, y_pred_knn))
print("\nKNN Classification Report:\n", classification_report(y_test, y_pred_knn))
print("\nKNN Accuracy Score:", accuracy_score(y_test, y_pred_knn))

"""Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

y_pred_nb = nb_model.predict(X_test)

print("Naive Bayes Confusion Matrix:\n", confusion_matrix(y_test, y_pred_nb))
print("\nNaive Bayes Classification Report:\n", classification_report(y_test, y_pred_nb))
print("\nNaive Bayes Accuracy Score:", accuracy_score(y_test, y_pred_nb))

"""Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

# No scaling needed for Decision Tree
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

y_pred_dt = dt_model.predict(X_test)

print("Decision Tree Confusion Matrix:\n", confusion_matrix(y_test, y_pred_dt))
print("\nDecision Tree Classification Report:\n", classification_report(y_test, y_pred_dt))
print("\nDecision Tree Accuracy Score:", accuracy_score(y_test, y_pred_dt))

"""XGBoost"""

from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# Note: No scaling strictly needed for XGBoost (tree-based)

# 1. Build and train XGBoost model
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_model.fit(X_train, y_train)

# 2. Predictions
y_pred_xgb = xgb_model.predict(X_test)

# 3. Evaluation
print("XGBoost Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))
print("\nXGBoost Classification Report:\n", classification_report(y_test, y_pred_xgb))
print("\nXGBoost Accuracy Score:", accuracy_score(y_test, y_pred_xgb))

#Find out which model is best
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=100),
    "SVM": SVC(kernel='rbf', gamma='scale'),
    "KNN": KNeighborsClassifier(),
    "Naive Bayes": GaussianNB(),
    "Decision Tree": DecisionTreeClassifier(),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss')
}

results = []
for name, model in models.items():
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
    results.append({
        "Model": name,
        "Mean Accuracy": scores.mean(),
        "Std Dev": scores.std()
    })

df_results = pd.DataFrame(results)
print(df_results.sort_values(by="Mean Accuracy", ascending=False))

#Cross Validate
from sklearn.model_selection import cross_validate

# Metrics to evaluate
scoring = ['accuracy', 'precision', 'recall', 'f1']

# Store results
results = []

# Loop through models and cross-validate
for name, model in models.items():
    scores = cross_validate(model, X, y, cv=5, scoring=scoring)
    results.append({
        'Model': name,
        'Accuracy': round(scores['test_accuracy'].mean(), 3),
        'Precision': round(scores['test_precision'].mean(), 3),
        'Recall': round(scores['test_recall'].mean(), 3),
        'F1-Score': round(scores['test_f1'].mean(), 3)
    })

# Display results
df_results = pd.DataFrame(results).sort_values(by='Accuracy', ascending=False)
print(df_results)

# Train Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Get feature importances
importances = rf_model.feature_importances_
feature_names = X_train.columns

# Create a DataFrame
feat_imp_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print(feat_imp_df)

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feat_imp_df, palette='viridis')
plt.title('Feature Importance from Random Forest')
plt.tight_layout()
plt.show()

#Plot a box plot for glucose
sns.boxplot(x='Outcome', y='Glucose', data=df)

from scipy import stats
import numpy as np

z_scores = stats.zscore(df['Glucose'])
outliers = df[np.abs(z_scores) > 2]  # or even 1.5 for more sensitivity
print(outliers['Glucose'])

#Box plot for BMI
sns.boxplot(x='Outcome', y='BMI', data=df)

#box plot for diabetespedigreefunction_scaled
sns.boxplot(x='Outcome', y='DiabetesPedigreeFunction_scaled', data=df)

#Box plot for age
sns.boxplot(x='Outcome', y='Age', data=df)

print(df.duplicated().sum())

sns.histplot(df['Glucose'], kde=True)
plt.show()

#histogram for bmi
sns.histplot(df['BMI'], kde=True)
plt.show()

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)


# Predict on Test set
y_pred_rf = rf.predict(X_test)
y_pred_rf_proba = rf.predict_proba(X_test)[:, 1]  # for ROC AUC

print("Random Forest Classifier Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))